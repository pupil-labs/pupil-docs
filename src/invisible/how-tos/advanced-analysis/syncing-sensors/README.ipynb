{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync with External Sensors\n",
    "\n",
    "Many experimental setups record data from multiple sensors in parallel. This data needs to be synced temporally for a joint analysis. All eye tracking data you record with Pupil Invisible is accurately timestamped, which makes this easily possible.\n",
    "\n",
    "In this guide you will learn how to sync eye tracking data from Pupil Invisible to any other timestamped external sensor using the `pd.merge_asof` function of Pandas. As an example, we will sync a heart rate sensor with a Pupil Invisible recording to produce a gaze overlay visualization with real-time heart rate of a man jogging.\n",
    "\n",
    "TODO\n",
    "- What heart rate sensor die we use?\n",
    "\n",
    "## Dependencies of this Guide\n",
    "To be written.\n",
    "- Required Python libs\n",
    "    - pandas av fitdecode tqdm opencv-python\n",
    "- Where to download the code\n",
    "- Where to download the example data\n",
    "- How to put data into data folder\n",
    "\n",
    "## Loading all Data\n",
    "For the example visualization we need the scene video and gaze data from the Pupil Invisible recording, and the heart rate data from the [??]() recording.\n",
    "\n",
    "The heart rate data can be read using the `fitdecode` module. For more details check out the implementation of the [load_fit_data]() implementation.\n",
    "\n",
    "The gaze data CSV file can be read using Pandas. All we need from it is the timestamps and the gaze values.\n",
    "\n",
    "For the scene video, we initially only need its timestamps and the corresponding frame indices for matching, we don't have to touch the actual video frames yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from decode_fit import load_fit_data\n",
    "\n",
    "\n",
    "heart_rate_path = \"data/eye-tracking-run.FIT\"\n",
    "hr = load_fit_data(heart_rate_path)\n",
    "hr = hr[[\"timestamp\", \"heart_rate\"]]\n",
    "\n",
    "gaze_path = \"data/demo-recording/running_rd-4a40d94d/gaze.csv\"\n",
    "gaze = pd.read_csv(gaze_path)\n",
    "gaze[\"timestamp [ns]\"] = pd.to_datetime(gaze[\"timestamp [ns]\"])\n",
    "gaze = gaze[[\"timestamp [ns]\", \"gaze x [px]\", \"gaze y [px]\"]]\n",
    "\n",
    "world_ts_path = \"data/demo-recording/running_rd-4a40d94d/world_timestamps.csv\"\n",
    "world_ts = pd.read_csv(world_ts_path)\n",
    "world_ts = world_ts[[\"timestamp [ns]\"]]\n",
    "world_ts[\"frame_index\"] = world_ts.index\n",
    "world_ts[\"timestamp [ns]\"] = pd.to_datetime(world_ts[\"timestamp [ns]\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp Matching\n",
    "\n",
    "The challenge with syncing the three data streams is that while they are all timestamped, their timestamps are not identical. Every stream is sampled independently and at different rates. E.g. the gaze data is sampled at 200 Hz, while the scene video is only sampled at 30 Hz, so there are a lot more gaze samples than video frames in our recording.\n",
    "\n",
    "The visualization we are going for includes a gaze overlay, so given a scene video frame we need to overlay it with a gaze sample. We have to decide between two options here: \n",
    "\n",
    "**1.** Given the timestamp of the video frame we search for the gaze samples that is closest in time and choose it for the overlay. This would imply that most of the gaze samples are not visible in the overlay, because there are more gaze samples than frames.\n",
    "\n",
    "**2.** We match every single gaze sample to its closest video frame. All the gaze samples that match to the same frame are averaged and this value is used for the overlay.\n",
    "\n",
    "For gaze data option 2 is usually better, as the averaging contributes a bit of noise reduction.\n",
    "\n",
    "The heart rate data is sampled much more sparsely with just 1 sample / minute. So for it we will choose option 1, which in this case means that the same heart rate sample will match to multiple video frames.\n",
    "\n",
    "Both options can be implemented using the Pandas function [`pd.merge_asof`](https://pandas.pydata.org/pandas-docs/version/0.25.0/reference/api/pandas.merge_asof.html). It merges two DataFrames based on indices that do not match perfectly by finding the closest matches, which is exactly what we need.\n",
    "\n",
    "### Matching Video and Gaze\n",
    "As mentioned above we will use option 2 for matching, i.e. we will match every gaze sample to the closest existing world frame, and then calculate the mean gaze value for every world frame. Note in the result below that the first couple of world frames do not have any matches. This is because the world camera initializes faster when starting a recording and thus records a couple seconds sooner than the eye cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp [ns]</th>\n",
       "      <th>frame_index</th>\n",
       "      <th>gaze x [px]</th>\n",
       "      <th>gaze y [px]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-09 17:19:08.694000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-09 17:19:08.744000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-09 17:19:08.794000000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-09 17:19:08.844000000</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-09 17:19:08.894000000</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74214</th>\n",
       "      <td>2022-07-09 18:00:53.466044444</td>\n",
       "      <td>74214</td>\n",
       "      <td>427.126000</td>\n",
       "      <td>880.560833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74215</th>\n",
       "      <td>2022-07-09 18:00:53.498311111</td>\n",
       "      <td>74215</td>\n",
       "      <td>424.746857</td>\n",
       "      <td>882.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74216</th>\n",
       "      <td>2022-07-09 18:00:53.534600000</td>\n",
       "      <td>74216</td>\n",
       "      <td>426.106167</td>\n",
       "      <td>884.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74217</th>\n",
       "      <td>2022-07-09 18:00:53.566655555</td>\n",
       "      <td>74217</td>\n",
       "      <td>427.429429</td>\n",
       "      <td>884.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74218</th>\n",
       "      <td>2022-07-09 18:00:53.598533333</td>\n",
       "      <td>74218</td>\n",
       "      <td>430.543800</td>\n",
       "      <td>885.591900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74219 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp [ns]  frame_index  gaze x [px]  gaze y [px]\n",
       "0     2022-07-09 17:19:08.694000000            0          NaN          NaN\n",
       "1     2022-07-09 17:19:08.744000000            1          NaN          NaN\n",
       "2     2022-07-09 17:19:08.794000000            2          NaN          NaN\n",
       "3     2022-07-09 17:19:08.844000000            3          NaN          NaN\n",
       "4     2022-07-09 17:19:08.894000000            4          NaN          NaN\n",
       "...                             ...          ...          ...          ...\n",
       "74214 2022-07-09 18:00:53.466044444        74214   427.126000   880.560833\n",
       "74215 2022-07-09 18:00:53.498311111        74215   424.746857   882.799000\n",
       "74216 2022-07-09 18:00:53.534600000        74216   426.106167   884.010000\n",
       "74217 2022-07-09 18:00:53.566655555        74217   427.429429   884.170000\n",
       "74218 2022-07-09 18:00:53.598533333        74218   430.543800   885.591900\n",
       "\n",
       "[74219 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match every gaze sample to the closest world timestamp\n",
    "gaze_world = pd.merge_asof(gaze, world_ts, left_on=\"timestamp [ns]\", right_on=\"timestamp [ns]\", direction=\"nearest\")\n",
    "\n",
    "# Calculate the mean gaze position for each world frame\n",
    "gaze_world = gaze_world.groupby(\"frame_index\").mean()\n",
    "\n",
    "# Merge the gaze data with the world frame data\n",
    "df = pd.merge(world_ts, gaze_world, left_on=\"frame_index\", right_on=\"frame_index\", how=\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Video and Heart Rate\n",
    "We will match video and heart rate data using option 1, i.e. we match every world frame to the closest heart rate sample. Note how the heart rate samples repeat themselves for several world frames as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp [ns]</th>\n",
       "      <th>frame_index</th>\n",
       "      <th>gaze x [px]</th>\n",
       "      <th>gaze y [px]</th>\n",
       "      <th>heart_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-09 17:19:08.694000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-09 17:19:08.744000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-09 17:19:08.794000000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-09 17:19:08.844000000</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-09 17:19:08.894000000</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74214</th>\n",
       "      <td>2022-07-09 18:00:53.466044444</td>\n",
       "      <td>74214</td>\n",
       "      <td>427.126000</td>\n",
       "      <td>880.560833</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74215</th>\n",
       "      <td>2022-07-09 18:00:53.498311111</td>\n",
       "      <td>74215</td>\n",
       "      <td>424.746857</td>\n",
       "      <td>882.799000</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74216</th>\n",
       "      <td>2022-07-09 18:00:53.534600000</td>\n",
       "      <td>74216</td>\n",
       "      <td>426.106167</td>\n",
       "      <td>884.010000</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74217</th>\n",
       "      <td>2022-07-09 18:00:53.566655555</td>\n",
       "      <td>74217</td>\n",
       "      <td>427.429429</td>\n",
       "      <td>884.170000</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74218</th>\n",
       "      <td>2022-07-09 18:00:53.598533333</td>\n",
       "      <td>74218</td>\n",
       "      <td>430.543800</td>\n",
       "      <td>885.591900</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74219 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp [ns]  frame_index  gaze x [px]  gaze y [px]  \\\n",
       "0     2022-07-09 17:19:08.694000000            0          NaN          NaN   \n",
       "1     2022-07-09 17:19:08.744000000            1          NaN          NaN   \n",
       "2     2022-07-09 17:19:08.794000000            2          NaN          NaN   \n",
       "3     2022-07-09 17:19:08.844000000            3          NaN          NaN   \n",
       "4     2022-07-09 17:19:08.894000000            4          NaN          NaN   \n",
       "...                             ...          ...          ...          ...   \n",
       "74214 2022-07-09 18:00:53.466044444        74214   427.126000   880.560833   \n",
       "74215 2022-07-09 18:00:53.498311111        74215   424.746857   882.799000   \n",
       "74216 2022-07-09 18:00:53.534600000        74216   426.106167   884.010000   \n",
       "74217 2022-07-09 18:00:53.566655555        74217   427.429429   884.170000   \n",
       "74218 2022-07-09 18:00:53.598533333        74218   430.543800   885.591900   \n",
       "\n",
       "       heart_rate  \n",
       "0              95  \n",
       "1              95  \n",
       "2              95  \n",
       "3              95  \n",
       "4              95  \n",
       "...           ...  \n",
       "74214         169  \n",
       "74215         169  \n",
       "74216         169  \n",
       "74217         169  \n",
       "74218         169  \n",
       "\n",
       "[74219 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match every world frame to the closest heart rate sample\n",
    "world_hr = pd.merge_asof(world_ts, hr, left_on=\"timestamp [ns]\", right_on=\"timestamp\", direction=\"nearest\")\n",
    "\n",
    "# We use the frame index to merge this into the data frame\n",
    "# so we can drop the timestamps here\n",
    "world_hr.drop([\"timestamp\", \"timestamp [ns]\"], axis=1, inplace=True)\n",
    "\n",
    "# Merge the matched heart rate data with the previous\n",
    "# data frame containing the world frame data and gaze\n",
    "df = pd.merge(df, world_hr, left_on=\"frame_index\", right_on=\"frame_index\", how=\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvenc not available h264_nvenc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\git\\pupil-docs\\src\\invisible\\how-tos\\advanced-analysis\\syncing-sensors\\README.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000011?line=2'>3</a>\u001b[0m world_video_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/demo-recording/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000011?line=3'>4</a>\u001b[0m visualization_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/visualization.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000011?line=6'>7</a>\u001b[0m make_visualization(df\u001b[39m.\u001b[39;49miloc[\u001b[39m10000\u001b[39;49m:], world_video_path, visualization_path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\git\\pupil-docs\\src\\invisible\\how-tos\\advanced-analysis\\syncing-sensors\\visualization.py:67\u001b[0m, in \u001b[0;36mmake_visualization\u001b[1;34m(matched_data, world_video_path, output_path)\u001b[0m\n\u001b[0;32m     65\u001b[0m new_frame\u001b[39m.\u001b[39mpts \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mpts\n\u001b[0;32m     66\u001b[0m new_frame\u001b[39m.\u001b[39mtime_base \u001b[39m=\u001b[39m original_video_stream\u001b[39m.\u001b[39mtime_base\n\u001b[1;32m---> 67\u001b[0m packets \u001b[39m=\u001b[39m visualization_video\u001b[39m.\u001b[39;49mencode(new_frame)\n\u001b[0;32m     68\u001b[0m progress\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m     69\u001b[0m visualization_container\u001b[39m.\u001b[39mmux(packets)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from visualization import make_visualization\n",
    "\n",
    "world_video_path = \"data/demo-recording/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4\"\n",
    "visualization_path = \"data/visualization.mp4\"\n",
    "\n",
    "\n",
    "make_visualization(df, world_video_path, visualization_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/marc/Downloads/raw-data-export (2)/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\git\\pupil-docs\\src\\invisible\\how-tos\\advanced-analysis\\syncing-sensors\\README.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000004?line=3'>4</a>\u001b[0m world_vid_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/marc/Downloads/raw-data-export (2)/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000004?line=4'>5</a>\u001b[0m world_lookup \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, packet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(av\u001b[39m.\u001b[39;49mopen(world_vid_path)\u001b[39m.\u001b[39mdemux(video\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000004?line=6'>7</a>\u001b[0m     world_lookup[index] \u001b[39m=\u001b[39m packet\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/git/pupil-docs/src/invisible/how-tos/advanced-analysis/syncing-sensors/README.ipynb#ch0000004?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39m==\u001b[39m cut_off_index:\n",
      "File \u001b[1;32mav\\container\\core.pyx:401\u001b[0m, in \u001b[0;36mav.container.core.open\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mav\\container\\core.pyx:272\u001b[0m, in \u001b[0;36mav.container.core.Container.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mav\\container\\core.pyx:292\u001b[0m, in \u001b[0;36mav.container.core.Container.err_check\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mav\\error.pyx:336\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/marc/Downloads/raw-data-export (2)/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4'"
     ]
    }
   ],
   "source": [
    "import av\n",
    "\n",
    "cut_off_index = 15000\n",
    "world_vid_path = \"/Users/marc/Downloads/raw-data-export (2)/running_rd-4a40d94d/3e2512bf_0.0-2504.94.mp4\"\n",
    "world_lookup = {}\n",
    "for index, packet in enumerate(av.open(world_vid_path).demux(video=0)):\n",
    "    world_lookup[index] = packet\n",
    "    \n",
    "    if index == cut_off_index:\n",
    "        break\n",
    "\n",
    "world_ts = world_ts.iloc[:cut_off_index]\n",
    "\n",
    "vis_vid_path = \"/Users/marc/Downloads/raw-data-export (2)/running_rd-4a40d94d/vis.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx < 30 * 60 * 10:\n",
    "        continue\n",
    "    \n",
    "    frame = world_lookup[row[\"frame_index\"]].decode()\n",
    "    try:\n",
    "        frame = frame[0]\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "    img = frame.to_ndarray(format=\"bgr24\")\n",
    "    \n",
    "    gaze = (int(row[\"gaze x [px]\"]), int(row[\"gaze y [px]\"]))\n",
    "    cv2.circle(img, gaze, 50, (0, 0, 255), 5)\n",
    "    \n",
    "    heart_rate = row[\"heart_rate\"]\n",
    "    cv2.putText(img, f\"HR: {heart_rate}\", (50,100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3)\n",
    "\n",
    "    cv2.imshow(\"Scene Video + Gaze + Heartrate\", img)\n",
    "    key = cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvenc not available h264_nvenc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/15000 [00:00<11:08, 22.40 frames/s] /var/folders/k3/8x0h0s854dq0jc83qmhmzn3w0000gn/T/ipykernel_3690/4244152212.py:16: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure()\n",
      "  5%|▍         | 743/15000 [00:46<14:50, 16.00 frames/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize(img, df, idx):\n",
    "    row = df.iloc[idx]\n",
    "    gaze = (int(row[\"gaze x [px]\"]), int(row[\"gaze y [px]\"]))\n",
    "    cv2.circle(img, gaze, 50, (0, 0, 255), 5)\n",
    "    \n",
    "    heart_rate = row[\"heart_rate\"]\n",
    "    cv2.putText(img, f\"HR: {heart_rate}\", (50,100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3)\n",
    "\n",
    "    # Make a random plot...\n",
    "    fig = plt.figure()\n",
    "    # ax = fig.add_subplot(111)\n",
    "\n",
    "    pad = 50\n",
    "    heart_rate = df.iloc[idx - pad: idx + pad].heart_rate\n",
    "\n",
    "    plt.plot(np.arange(len(heart_rate)) + idx - pad, heart_rate, color=\"blue\")\n",
    "    plt.xlim(idx - pad, idx + pad)\n",
    "\n",
    "    # If we haven't already shown or saved the plot, then we need to\n",
    "    # draw the figure first...\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Now we can save it to a numpy array.\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    w_v, h_v, _ = data.shape\n",
    "    w, h, _ = img.shape\n",
    "\n",
    "    img[w-w_v:w, 0:h_v, :] = data\n",
    "\n",
    "    return img\n",
    "\n",
    "original_container = av.open(str(world_vid_path))\n",
    "original_video_stream = original_container.streams.video[0]\n",
    "\n",
    "visualization_container = av.open(str(vis_vid_path), \"w\")\n",
    "\n",
    "try:\n",
    "    visualization_video = visualization_container.add_stream(\"h264_nvenc\")\n",
    "except Exception as e:\n",
    "    print(\"nvenc not available\", e)\n",
    "    visualization_video = visualization_container.add_stream(\"h264\")\n",
    "\n",
    "visualization_video.options[\"bf\"] = \"0\"\n",
    "visualization_video.options[\"movflags\"] = \"faststart\"\n",
    "visualization_video.gop_size = original_video_stream.gop_size\n",
    "visualization_video.codec_context.height = original_video_stream.height\n",
    "visualization_video.codec_context.width = original_video_stream.width\n",
    "visualization_video.codec_context.time_base = original_video_stream.time_base\n",
    "visualization_video.codec_context.bit_rate = original_video_stream.bit_rate\n",
    "\n",
    "progress = tqdm(unit=\" frames\", total=len(df))\n",
    "with visualization_container:\n",
    "    for idx, row in df.iterrows():\n",
    "        frame = world_lookup[row[\"frame_index\"]].decode()\n",
    "        try:\n",
    "            frame = frame[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        img = frame.to_ndarray(format=\"bgr24\")\n",
    "        vis_img = visualize(img, df, idx)\n",
    "\n",
    "        # cv2.imshow(\"Scene Video + Gaze + Heartrate\", vis_img)\n",
    "        # cv2.waitKey(1)\n",
    "        \n",
    "        new_frame = frame.from_ndarray(vis_img, format=\"bgr24\")\n",
    "        new_frame.pts = frame.pts\n",
    "        new_frame.time_base = original_video_stream.time_base\n",
    "        packets = visualization_video.encode(new_frame)\n",
    "        progress.update()\n",
    "        visualization_container.mux(packets)\n",
    "    # encode and mux frames that have been queued internally by the encoders\n",
    "    visualization_container.mux(visualization_video.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sensor-sync')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d87e97d5eb72e6239d630360e86afce37607673c01853624d91291a4f555ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
